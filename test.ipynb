{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445c7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Utility: Check if finger is up\n",
    "def is_finger_up(landmarks, tip_id, pip_id):\n",
    "    return landmarks[tip_id].y < landmarks[pip_id].y\n",
    "\n",
    "# Gesture detection\n",
    "def classify_gesture(landmarks):\n",
    "    fingers = []\n",
    "\n",
    "    # Thumb\n",
    "    fingers.append(landmarks[4].x < landmarks[3].x)  # Left hand assumption\n",
    "\n",
    "    # Fingers (Index, Middle, Ring, Pinky)\n",
    "    fingers.append(is_finger_up(landmarks, 8, 6))   # Index\n",
    "    fingers.append(is_finger_up(landmarks, 12, 10)) # Middle\n",
    "    fingers.append(is_finger_up(landmarks, 16, 14)) # Ring\n",
    "    fingers.append(is_finger_up(landmarks, 20, 18)) # Pinky\n",
    "\n",
    "    if fingers == [False, True, False, False, False]:\n",
    "        return \"One Finger\"\n",
    "    elif fingers == [False, False, False, False, False]:\n",
    "        return \"Fist\"\n",
    "    elif fingers == [True, False, False, False, False]:\n",
    "        return \"Thumbs Up\"\n",
    "    elif fingers == [True, True, True, True, True]:\n",
    "        return \"Open Palm\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            gesture = classify_gesture(hand_landmarks.landmark)\n",
    "\n",
    "            # Show gesture on screen\n",
    "            cv2.putText(frame, f\"Gesture: {gesture}\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 0), 2)\n",
    "\n",
    "            # Trigger actions\n",
    "            if gesture == \"One Finger\":\n",
    "                pyautogui.press(\"space\")  # Play/Pause\n",
    "            elif gesture == \"Fist\":\n",
    "                pyautogui.press(\"s\")  # Stop\n",
    "            elif gesture == \"Thumbs Up\":\n",
    "                pyautogui.press(\"up\")  # Like/Volume up\n",
    "            elif gesture == \"Open Palm\":\n",
    "                pyautogui.press(\"r\")  # Reset\n",
    "\n",
    "    cv2.imshow(\"HandyPanda üêº\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06855bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Utility Functions\n",
    "def is_finger_up(landmarks, tip_id, pip_id):\n",
    "    return landmarks[tip_id].y < landmarks[pip_id].y\n",
    "\n",
    "def is_finger_folded(landmarks, tip_id, pip_id):\n",
    "    return landmarks[tip_id].y > landmarks[pip_id].y\n",
    "\n",
    "def is_thumb_up(landmarks):\n",
    "    return landmarks[4].y < landmarks[3].y < landmarks[2].y\n",
    "\n",
    "# Gesture Classifier\n",
    "def classify_gesture(landmarks):\n",
    "    thumb_up = is_thumb_up(landmarks)\n",
    "    index_up = is_finger_up(landmarks, 8, 6)\n",
    "    middle_folded = is_finger_folded(landmarks, 12, 10)\n",
    "    ring_folded = is_finger_folded(landmarks, 16, 14)\n",
    "    pinky_folded = is_finger_folded(landmarks, 20, 18)\n",
    "    index_folded = is_finger_folded(landmarks, 8, 6)\n",
    "\n",
    "    if thumb_up and index_folded and middle_folded and ring_folded and pinky_folded:\n",
    "        return \"Thumbs Up üëç\"\n",
    "    \n",
    "    if index_up and middle_folded and ring_folded and pinky_folded:\n",
    "        return \"Index Pointing üëâ\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# OpenCV Webcam Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip and convert color\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process frame\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    gesture = \"Waiting...\"\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            landmarks = hand_landmarks.landmark\n",
    "            gesture = classify_gesture(landmarks)\n",
    "\n",
    "            # Display gesture\n",
    "            cv2.putText(frame, f'Gesture: {gesture}', (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 100), 2)\n",
    "\n",
    "    # Display output\n",
    "    cv2.imshow(\"HandyPanda üêº\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b37289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe and PyAutoGUI\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Screen size for mapping\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "prev_y = None\n",
    "click_threshold = 0.05  # Distance threshold for click\n",
    "scroll_sensitivity = 0.01 # Sensitivity for scroll : higher = less sensitive\n",
    "# Distance utility\n",
    "def get_distance(p1, p2):\n",
    "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2)\n",
    "\n",
    "# Webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            landmarks = hand_landmarks.landmark\n",
    "\n",
    "            # --- Mouse Movement ---\n",
    "            index_tip = landmarks[8]\n",
    "            x = int(index_tip.x * screen_w)\n",
    "            y = int(index_tip.y * screen_h)\n",
    "            pyautogui.moveTo(x, y, duration=0.01)\n",
    "\n",
    "            # --- Click Gesture (thumb & index touch) ---\n",
    "            thumb_tip = landmarks[4]\n",
    "            distance = get_distance(thumb_tip, index_tip)\n",
    "\n",
    "            if distance < click_threshold:\n",
    "                pyautogui.click()\n",
    "                cv2.putText(frame, \"Click!\", (10, 80),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 100, 255), 3)\n",
    "\n",
    "            # --- Scroll Gesture (wrist movement) ---\n",
    "            wrist_y = landmarks[0].y\n",
    "            if prev_y is not None:\n",
    "                delta = wrist_y - prev_y\n",
    "                if abs(delta) > scroll_sensitivity:\n",
    "                    direction = int(-delta * 100)\n",
    "                    pyautogui.scroll(direction)\n",
    "            prev_y = wrist_y\n",
    "\n",
    "    cv2.imshow(\"HandyPanda üêº Mouse Control\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfb79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe and PyAutoGUI\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Screen size\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "prev_y = None\n",
    "scroll_sensitivity = 0.03\n",
    "click_threshold = 0.05\n",
    "\n",
    "# Distance utility\n",
    "def get_distance(p1, p2):\n",
    "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2)\n",
    "\n",
    "# Finger state checkers\n",
    "def is_finger_up(landmarks, tip_id, pip_id):\n",
    "    return landmarks[tip_id].y < landmarks[pip_id].y\n",
    "\n",
    "def is_finger_folded(landmarks, tip_id, pip_id):\n",
    "    return landmarks[tip_id].y > landmarks[pip_id].y\n",
    "\n",
    "def is_fist(landmarks):\n",
    "    return all(is_finger_folded(landmarks, tip, pip)\n",
    "               for tip, pip in [(8, 6), (12, 10), (16, 14), (20, 18)])\n",
    "\n",
    "def is_index_pointing(landmarks):\n",
    "    return (is_finger_up(landmarks, 8, 6) and\n",
    "            all(is_finger_folded(landmarks, tip, pip)\n",
    "                for tip, pip in [(12, 10), (16, 14), (20, 18)]))\n",
    "\n",
    "# Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    action_text = \"Idle\"\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = hand_landmarks.landmark\n",
    "\n",
    "            index_tip = landmarks[8]\n",
    "            thumb_tip = landmarks[4]\n",
    "            middle_tip = landmarks[12]\n",
    "\n",
    "            # Click Gestures\n",
    "            distance_thumb_index = get_distance(index_tip, thumb_tip)\n",
    "            distance_thumb_middle = get_distance(thumb_tip, middle_tip)\n",
    "\n",
    "            if distance_thumb_index < click_threshold:\n",
    "                pyautogui.click()\n",
    "                action_text = \"Left Click\"\n",
    "\n",
    "            elif distance_thumb_middle < click_threshold:\n",
    "                pyautogui.rightClick()\n",
    "                action_text = \"Right Click\"\n",
    "\n",
    "            elif is_fist(landmarks):\n",
    "                # --- Scroll Mode ---\n",
    "                wrist_y = landmarks[0].y\n",
    "                if prev_y is not None:\n",
    "                    delta = wrist_y - prev_y\n",
    "                    if abs(delta) > scroll_sensitivity:\n",
    "                        direction = int(-delta * 100)\n",
    "                        pyautogui.scroll(direction)\n",
    "                        action_text = \"Scrolling\"\n",
    "                prev_y = wrist_y\n",
    "\n",
    "            elif is_index_pointing(landmarks):\n",
    "                # --- Mouse Movement Mode ---\n",
    "                x = int(index_tip.x * screen_w)\n",
    "                y = int(index_tip.y * screen_h)\n",
    "                pyautogui.moveTo(x, y, duration=0.01)\n",
    "                action_text = \"Moving Mouse\"\n",
    "                prev_y = None  # reset scroll tracking\n",
    "\n",
    "            else:\n",
    "                action_text = \"Idle\"\n",
    "                prev_y = None\n",
    "\n",
    "    # Show action label\n",
    "    cv2.putText(frame, f'Mode: {action_text}', (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 150), 2)\n",
    "\n",
    "    cv2.imshow(\"HandyPanda üêº\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc48474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Initialize\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "prev_y = None\n",
    "click_threshold = 0.05\n",
    "scroll_sensitivity = 0.02\n",
    "\n",
    "# State flag\n",
    "scroll_mode = False\n",
    "\n",
    "# Distance helper\n",
    "def get_distance(p1, p2):\n",
    "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2)\n",
    "\n",
    "# Finger logic\n",
    "def is_finger_folded(landmarks, tip_id, pip_id):\n",
    "    return landmarks[tip_id].y > landmarks[pip_id].y\n",
    "\n",
    "def all_fingers_folded(landmarks):\n",
    "    folded = [is_finger_folded(landmarks, tip, pip)\n",
    "              for tip, pip in [(8, 6), (12, 10), (16, 14), (20, 18)]]\n",
    "    return all(folded)\n",
    "\n",
    "def is_index_only_up(landmarks):\n",
    "    return (landmarks[8].y < landmarks[6].y and\n",
    "            all(is_finger_folded(landmarks, tip, pip)\n",
    "                for tip, pip in [(12, 10), (16, 14), (20, 18)]))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    action_text = \"Idle\"\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = hand_landmarks.landmark\n",
    "\n",
    "            index_tip = landmarks[8]\n",
    "            thumb_tip = landmarks[4]\n",
    "            middle_tip = landmarks[12]\n",
    "\n",
    "            # Gesture distances\n",
    "            distance_thumb_index = get_distance(index_tip, thumb_tip)\n",
    "            distance_thumb_middle = get_distance(thumb_tip, middle_tip)\n",
    "\n",
    "            # Clicks\n",
    "            if distance_thumb_index < click_threshold:\n",
    "                pyautogui.click()\n",
    "                action_text = \"Left Click\"\n",
    "                scroll_mode = False\n",
    "                continue\n",
    "\n",
    "            if distance_thumb_middle < click_threshold:\n",
    "                pyautogui.rightClick()\n",
    "                action_text = \"Right Click\"\n",
    "                scroll_mode = False\n",
    "                continue\n",
    "\n",
    "            # Enter/exit scroll mode\n",
    "            if all_fingers_folded(landmarks):\n",
    "                if not scroll_mode:\n",
    "                    scroll_mode = True\n",
    "                    prev_y = landmarks[0].y\n",
    "                else:\n",
    "                    wrist_y = landmarks[0].y\n",
    "                    delta = wrist_y - prev_y\n",
    "                    if abs(delta) > scroll_sensitivity:\n",
    "                        pyautogui.scroll(int(-delta * 100))\n",
    "                        action_text = \"Scrolling\"\n",
    "                        prev_y = wrist_y\n",
    "            else:\n",
    "                scroll_mode = False\n",
    "                prev_y = None\n",
    "\n",
    "            # Mouse movement\n",
    "            if is_index_only_up(landmarks) and not scroll_mode:\n",
    "                x = int(index_tip.x * screen_w)\n",
    "                y = int(index_tip.y * screen_h)\n",
    "                pyautogui.moveTo(x, y, duration=0.01)\n",
    "                action_text = \"Moving Mouse\"\n",
    "\n",
    "    # Display state\n",
    "    cv2.putText(frame, f'Mode: {action_text}', (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 150), 2)\n",
    "\n",
    "    cv2.imshow(\"HandyPanda üêº\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e44114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "prev_y = None\n",
    "scroll_sensitivity = 0.02\n",
    "click_threshold = 0.05\n",
    "scroll_mode = False\n",
    "\n",
    "# Calibration Box (portion of webcam frame to use)\n",
    "calib_left = 0.2\n",
    "calib_top = 0.2\n",
    "calib_right = 0.8\n",
    "calib_bottom = 0.8\n",
    "\n",
    "# Smoothing factor\n",
    "smooth_factor = 5\n",
    "prev_x, prev_y_mouse = 0, 0\n",
    "\n",
    "def get_distance(p1, p2):\n",
    "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2)\n",
    "\n",
    "def is_finger_folded(landmarks, tip, pip):\n",
    "    return landmarks[tip].y > landmarks[pip].y\n",
    "\n",
    "def all_fingers_folded(landmarks):\n",
    "    return all(is_finger_folded(landmarks, tip, pip)\n",
    "               for tip, pip in [(8, 6), (12, 10), (16, 14), (20, 18)])\n",
    "\n",
    "def is_index_only_up(landmarks):\n",
    "    return (landmarks[8].y < landmarks[6].y and\n",
    "            all(is_finger_folded(landmarks, tip, pip)\n",
    "                for tip, pip in [(12, 10), (16, 14), (20, 18)]))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    action_text = \"Idle\"\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = hand_landmarks.landmark\n",
    "\n",
    "            index_tip = landmarks[8]\n",
    "            thumb_tip = landmarks[4]\n",
    "            middle_tip = landmarks[12]\n",
    "\n",
    "            dist_thumb_index = get_distance(index_tip, thumb_tip)\n",
    "            dist_thumb_middle = get_distance(thumb_tip, middle_tip)\n",
    "\n",
    "            if dist_thumb_index < click_threshold:\n",
    "                pyautogui.click()\n",
    "                action_text = \"Left Click\"\n",
    "                scroll_mode = False\n",
    "                continue\n",
    "\n",
    "            if dist_thumb_middle < click_threshold:\n",
    "                pyautogui.rightClick()\n",
    "                action_text = \"Right Click\"\n",
    "                scroll_mode = False\n",
    "                continue\n",
    "\n",
    "            if all_fingers_folded(landmarks):\n",
    "                if not scroll_mode:\n",
    "                    scroll_mode = True\n",
    "                    prev_y = landmarks[0].y\n",
    "                else:\n",
    "                    wrist_y = landmarks[0].y\n",
    "                    delta = wrist_y - prev_y\n",
    "                    if abs(delta) > scroll_sensitivity:\n",
    "                        pyautogui.scroll(int(-delta * 100))\n",
    "                        action_text = \"Scrolling\"\n",
    "                        prev_y = wrist_y\n",
    "            else:\n",
    "                scroll_mode = False\n",
    "                prev_y = None\n",
    "\n",
    "            if is_index_only_up(landmarks) and not scroll_mode:\n",
    "                x_norm = np.clip((index_tip.x - calib_left) / (calib_right - calib_left), 0, 1)\n",
    "                y_norm = np.clip((index_tip.y - calib_top) / (calib_bottom - calib_top), 0, 1)\n",
    "\n",
    "                x = int(x_norm * screen_w)\n",
    "                y = int(y_norm * screen_h)\n",
    "\n",
    "                # Smooth movement\n",
    "                curr_x = prev_x + (x - prev_x) // smooth_factor\n",
    "                curr_y = prev_y_mouse + (y - prev_y_mouse) // smooth_factor\n",
    "                pyautogui.moveTo(curr_x, curr_y, duration=0.01)\n",
    "                prev_x, prev_y_mouse = curr_x, curr_y\n",
    "\n",
    "                action_text = \"Moving Mouse\"\n",
    "\n",
    "    # Draw calibration rectangle\n",
    "    cv2.rectangle(frame,\n",
    "                  (int(calib_left * frame_w), int(calib_top * frame_h)),\n",
    "                  (int(calib_right * frame_w), int(calib_bottom * frame_h)),\n",
    "                  (255, 0, 255), 2)\n",
    "\n",
    "    cv2.putText(frame, f'Mode: {action_text}', (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 150), 2)\n",
    "\n",
    "    cv2.imshow(\"HandyPanda üêº\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
